cirm = 0.106 * age
cirm
#Calculating Confidence intervals in R
head(ToothGrowth)
##Confidence interval of mean
s = sd(ToothGrowth$len)
##Confidence interval of mean
s = sd(ToothGrowth$len)
standardError = s / sqrt(n)
zvalue = qnorm(0.975)
##Confidence interval of mean
n = len(ToothGrowth$len)
s = sd(ToothGrowth$len)
##Confidence interval of mean
n = length(ToothGrowth$len)
s = sd(ToothGrowth$len)
standardError = s / sqrt(n)
zvalue = qnorm(0.975)
zvalue
#Margin of error
moe = zvalue * standardError
xbar + c(-moe,moe)
xbar = mean(ToothGrowth$len)
xbar + c(-moe,moe)
tval = qt(0.975, df=n-1)
tval
zvalue
t.test(ToothGrowth$len) #95% ci for mean
t.test(ToothGrowth$len,conf.level = 0.9)
t.test(ToothGrowth$len,conf.level = 0.9)
t.test(ToothGrowth$len) #95% ci for mean
library(ggplot2)
#Statistical model is estimating parameter from a dataset.
fit = lm(circumference ~ age , data = Orange)
summary(fit)
ggplot(Orange, aes(x=age,y=circumference)) +
geom_point(color="#2990B9",size = 4) +
geom_smooth(method=lm, color = "#2C3E50")
new.data = data.frame(age=1500)
predict(fit,newdata = new.data, interval = "conf")
confint(fit)
summary(fit0)
#We can force intercept to be zero if we are sure that it is
#not significant
fit0 = lm(circumference ~ age + 0, data = Orange)
summary(fit0)
#RElationship between Anova and linear regression
fir = lm(Sepal.Length ~ Petal.Length, data=iris)
#RElationship between Anova and linear regression
fit1 = lm(Sepal.Length ~ Petal.Length, data=iris)
summary(fit1)
anova(fit1)
#Multiple Linear Regression
#One Response variable and two or more predictore
data(iris)
head(iris)
fit1 = lm(Sepal.Length ~ Sepal.Width + Petal.Length, data = iris)
summary(fit1)
fit2 = lm(Sepal.Length ~ Sepal.Width + Petal.Length + Petal.Width, data = iris)
summary(fit2)
qplot(Sepal.Length, Petal.Length, data = iris)
fitlm(Petal.Length ~ Sepal.Length, data=iris)
summary(fitlm)
qplot(Sepal.Length, Petal.Length, data = iris)
fitlm(Petal.Length ~ Sepal.Length, data=iris)
summary(fitlm)
#Addition of categorical variables
#Accounting for interaction of predictor variables.
data(iris)
library(ggplot2)
qplot(Sepal.Length, Petal.Length, data = iris)
fitlm(Petal.Length ~ Sepal.Length, data=iris)
View(iris)
summary(fitlm)
fitlm = lm(Petal.Length ~ Sepal.Length, data=iris)
summary(fitlm)
summary(iris)
qplot(Sepal.length, Petal.Length, data=iris, color = Species)
qplot(Sepal.Length, Petal.Length, data=iris, color = Species)
x = lm(Petal.Length, Sepal.Length+Species, data = iris)
summary(x)
x = lm(Petal.Length ~ Sepal.Length+Species, data = iris)
summary(x)
#Is there a signiicant variation in petal length accross species
#
fit1 = lm(Petal.Length ~ Sepal.Length* Species, data = iris)
anova(fit1)
#anova show that the Species affects the Petal.Length at
#a significanct level i.e. predictor affecss the response
summary(iris$Species
)
summary(fit1)
#Check for conditions met for OLS regression
fit = lm(Sepal.Length ~ Petal.Length + Petal.Width, data=iris)
summary(fit)
par(mfrow=c(2,2))
plot(fit)
fir = lm(Sepal.Length ~ Petal.Length + Petal.Width, data=iris)
summary(fit)
plot(fit)
par(mfrow=c(2,2))
plot(fit)
install.packages('lmtest')
library('lmtest')
dwtest(fit)
install.packages('cat')
install.packages('car')
dwtest(fit)
library(car)
ncvTest(fit)
#Identify outliers that have too much influence of model
#influential datapoints
cutoff = 4 /((nrow(iris) - length(fit$coefficients)-2))
plot(fit, which = 4,cook.levels = cutoff)
par(mfrow=c(1,1))
plot(fit, which = 4,cook.levels = cutoff)
install.packages('mlbench')
library('mlbench')
library(help = mlbench)
data('Bostonhousing')
data(Bostonhousing)
data(BostonHousing)
str('BostonHousing')
str(BostonHousing)
summary(BostonHousing)
head(BostonHousing)
library(ggplot2)
library(car)
library(caret)
install.packages('carer')
install.packages('caret')
install.packages('corrplot')
library(caret)
library(corrplot)
#Dropping response variable Y
mat_a = subset(BostonHousing, select -c(medv))
#Dropping response variable Y
mat_a = subset(BostonHousing, select = -c(medv))
str(mat_a)
numeric <- mat_a[sapply(mat_a,is.numeric)]
#Calculating corrlation between two variables
numeric
print(descrCor)
#Calculating corrlation between two variables
descrCor <- cor(numeric)
print(descrCor)
corrplot(descrCor)
#Remove highly correlated variables
highlyCorrelated = findCorrelation(descrCor, cutoff = 0.7)
highlyCorCol
highlyCorCol = colnames(numeric)[highlyCorrelated]
highlyCorCol
#Remove highly correlated variables from the original dataset
#Create a new dataset.
data3 = BostonHousing[,-which(colnames(BostonHousing) %in% highlyCorCol)]
dim(data3)
names(data3)
corrplot(data3)
corrplot(cor(data3))
cor(data3)
summary(fit)
#Variance inflation factor(VIF) can handle co-linearity as well
fit = lm(medv~., data = BostonHousing )
summary(fit)
vif(fit)
##Check
df = cbind(BostonHousing$medv, data3)
df = as.data.frame(df)
fit2 = lm(medv~.,data=df)
vif(fit2)
data("BostonHousing")
library(mlbench
)
data("BostonHousing")
str(BostonHousing)
require(pls)
install.packages('pls')
require(pls)
set.seed(1000)
pcr_mode <- pcr(medv~.,data="Boston Housing", scale=TRUE,validation="CV" )
pcr_mode <- pcr(medv~.,data="Boston Housing", scale=TRUE,validation="CV" )
pcr_mode <- pcr(medv~.,data="BostonHousing", scale=TRUE,validation="CV" )
pcr_mode <- pcr(medv~.,data=BostonHousing,scale=TRUE,validation="CV" )
pcr_model <- pcr(medv~.,data=BostonHousing,scale=TRUE,validation="CV" )
summary(pcr_model)
validationplot(pcr_model)
validationplot(pcr_model,val.type = "R2")
predplot(pcr_model)
#Partial Least Square Regresssion
library(caret)
control <- trainControl('repeatedcv',index=myfolds, selectionFunction = "oneSE" )
myfolds <- createMultiFolds(BostonHousing$medv, k=5, times = 10)
control <- trainControl('repeatedcv',index=myfolds, selectionFunction = "oneSE" )
#Train the PLS model
mod1 <- train(medv ~ . , data = BostonHousing,
method="pls",
metric = "R2",
tuneLength = 20,
trControl = control,
preProc= c("zv", "center", "scale"))
plot(mod1)
install.packages('plsdepot')
##Try another dataset
libray(plsdepot)
##Try another dataset
library(plsdepot)
data(vehicles)
head(vehicles)
names(vehicles)
cars = vehicles [,c(1:13,15:16,13)]
]
cars = vehicles [,c(1:12,14:16,13)]
pls1 = plsreg1(cars[,1:15],cars[,16, drop=FALSE], comps = 5)
pls1
pls1$R2
data("BostonHousing")
require('tidyverse','broom','glmnet')
require(c('tidyverse','broom','glmnet'))
require(c('tidyverse','broom','glmnet'))
install.packages('tidyverse','broom','glmnet')
install.packages('tidyverse')
install.packages('broom')
install.packages('glmnet')
library('tidyverse','broom','glmnet')
library('tidyverse')
library('broom')
library('glmnet')
lamdas = 10 ^ seq(3, -2, by = .1)
lamdas = 10 ^ seq(3, -2, by = -.1)
lambdas
lambdas = 10 ^ seq(3, -2, by = -.1)
lambdas
y = BostonHousing$medv
x = BostonHousing %>%
select(crim,zn,indus,chas,nox,rm,age,dis,rad,tax,ptratio,b,lstat)
%>% data.matrix()
x = BostonHousing %>%
select(crim,zn,indus,chas,nox,rm,age,dis,rad,tax,ptratio,b,lstat)%>% data.matrix()
x = BostonHousing %>%select(crim,zn,indus,chas,nox,rm,age,dis,rad,tax,ptratio,b,lstat)%>% data.matrix()
fit <- glmnet(x,y,alpha = 0, lambda = lambdas)
fir
fit
#What is the optimal lambda value
cv_fit = cv.glmnet(x,y,alpha=0, lambda = lambdas)
plot(cv_fit)
optlambda <- cv_fit$lambda.min
optlambda
#predicting values and computing an R2 values for the data we trained on
y_predicted <- predict(fit, s = optlambda, newx= x )
#Sum of squares total and error
sst <- sum(y^2)
sse <- sum((y_predicted -y)^2)
rsq <- 1 - sse / sst
rsq
data("BostonHousing")
#Lasso finds the optimum lamda values
y = BostonHousing$medv
x = BostonHousing %>%select(crim,zn,indus,chas,nox,rm,age,dis,rad,tax,ptratio,b,lstat)%>% data.matrix()
install.packages(MASS)
install.packages('MASS')
install.packages("MASS")
install.packages("MASS")
fit =lm(Sepal.Length~., data=iris)
#To examine all models
step(lm(mpg~wt + drat + disp + qsec,data=mtcars),direction = "both")
#To examine all models
step(lm(mpg~wt + drat + disp + qsec,data=mtcars),direction = "both")
step(lm(mpg~wt + drat + disp + qsec,data=mtcars),direction = "backward")
#To examine all models
step(lm(mpg~wt + drat + disp + qsec,data=mtcars),direction = "both")
step(lm(mpg~wt + drat + disp + qsec,data=mtcars),direction = "backward")
step(null, scope = list(lower=null,upper = full), direction = "both
")
null = lm(SepalLength~1, data = iris)
#Mean value of Y predicts new Ys
full = lm(Sepal.Length~., data = iris) #include all Xs
step(null, scope = list(lower=null,upper = full), direction = "both")
step(null, scope = list(lower=null,upper = full), direction = "both")
null = lm(SepalLength~1, data = iris)
null = lm(Sepal.Length~1, data = iris)
#Mean value of Y predicts new Ys
full = lm(Sepal.Length~., data = iris) #include all Xs
step(null, scope = list(lower=null,upper = full), direction = "both")
install.packages('relaimpo')
library('relaimpo')
b
b
boot <- boot.relimp(fit, b = 1000, type = c("lmg","last","first","pratt"),
rank=TRUE,diff = TRUE , rela= TRUE)
#Bootstrap Measures of Relative importance
#Drawing randomly with replacement from a set of data points
boot <- boot.relimp(fit, b = 1000, type = c("lmg","last","first","pratt"),
rank=TRUE,diff = TRUE , rela= TRUE)
fit =lm(formula = Sepal.Length~ Petal.Length + Sepal.Width +Petal.Width, data=iris)
#Bootstrap Measures of Relative importance
#Drawing randomly with replacement from a set of data points
boot <- boot.relimp(fit, b = 1000, type = c("lmg","last","first","pratt"),
rank=TRUE,diff = TRUE , rela= TRUE)
booteval.relimp(boot)
plot(booteval.relimp(boot, sort=TRUE))
data(BostonHousing)
fit1 = lm(medv~.,data = BostonHousing)
summary(fit1)
fit2 = lm(medv~lstat, data=BostonHousing)
fit2 = lm(medv~lstat, data=BostonHousing)
summary(fit2)
install.packages(leaps)
install.packages('leaps')
library(leaps)
regFit = regsubsets(medv~.,data=BostonHousing)
summary(regFit)
reg.summary = summary(regFit)
reg.summary$rsq
plot(regFit, scale="adjr2", main="Adjusted R ^ 2")
#Evaluating the accuracy of a regression model-A machine learning approach
data("BostonHousing")
#Evaluating the accuracy of a regression model-A machine learning approach
data(BostonHousing)
#Evaluating the accuracy of a regression model-A machine learning approach
data(BostonHousing)
#Evaluating the accuracy of a regression model-A machine learning approach
data(BostonHousing)
#Evaluating the accuracy of a regression model-A machine learning approach
data(BostonHousing)
#Evaluating the accuracy of a regression model-A machine learning approach
data(BostonHousing)
#Evaluating the accuracy of a regression model-A machine learning approach
data('BostonHousing')
#Evaluating the accuracy of a regression model-A machine learning approach
data('BostonHousing')
#Evaluating the accuracy of a regression model-A machine learning approach
data("BostonHousing")
#Evaluating the accuracy of a regression model-A machine learning approach
data(BostonHousing)
str(BostonHousing)
summary(BostonHousing)
library('caret')
set.seed(99)
Train = createDataPartition(BostonHousing$medv,p=0.75,list = FALSE)
#the ~. is to include all the predictor variables
fit1 = train(medv~., data=training, method="lm")
set.seed(99)
Train = createDataPartition(BostonHousing$medv,p=0.75,list = FALSE)
#75 25 SPLIT
training = BostonHousing[Train,]
testing = BostonHousing[-Train,]
#the ~. is to include all the predictor variables
fit1 = train(medv~., data=training, method="lm")
summary(fit1)
fit2 = train(medv~.,data=training,method = "lm", metric="RMSE")
print(fit2)
p1 = predict(fit2,newdata = testing)
p = as.data.frame(p1)
test = as.data.frame(testing$medv)
y = cbind(test,p)
colnames(y)=c("medv","Pred")
head(y)
cor.test(y$medv,y$Pred)
library(Metrics)
install.packages()
install.packages('Metrics')
library(Metrics)
rmse(y$medv, y$Pred)
train_control <- trainControl(method = "cv",number = 10)
fit3= train(medv~., data=BostonHousing,trControl=train_control, method="lm",metric="RMSE")
summary(fit3)
print(fit3)
train_control <- trainControl(method="cv",number = 10)
fit4 = train(medv~., data = training, trControl = train_control,method="lm")
print(fit4)
print(fit3)
colnames(y) = c("medv","Pred")
head(y)
cor.test(y$medv,y$Pred)
rmse(y$medv,y$Pred)
#LASSO REGRESSION FOR VARIABLE SELECTION
data(BostonHousing)
library(caret)
lasso <- train(medv~., BostonHousing,
method="lasso",
preProc = c('scale','center'),
trControl = train_control)
lasso
#Get Coefficient
predict.enet(lasso$finalModel,type='coefficients', s=lasso$bestTune$fraction)
library(relaimpo)
fit1 = lm(medv~.,data=BostonHousing)
calc.relimp(fit1, type=c("lmg"), rela = TRUE)
library(hier.part)
install.packages('hier.part')
x = BostonHousing[,1:12]
H = hier.part(BostonHousing$medv, x , fam="gaussian", gof = "Rsqu")
H = hier.part(BostonHousing$medv, x , fam="gaussian", gof = "Rsqu")
library(hier.part)
library(hier.part)
library(relaimpo)
install.packages('hier.part')
install.packages('hier.part')
library(hier.part)
library(hier.part)
install.packages('jier.part')
install.packages('hier.part')
install.packages('hier.part')
install.packages('hier.part',dependencies = TRUE)
#Boosted GAM
#Boosted Generalized additive models
library(caret)
library(mboost)
install.packages('libcoin')
library(mboost)
library(mboost)
eco = read.csv("biocap.csv")
#Generalized additive models are for non linear interaction
#between the variables.
setwd("C:/Users/kora/Documents/statistics/Regression Analysis for Statistics and Machine Learning in R [FCO]/R-Scripts-Regression-Analysis")
(
eco = read.csv('biocap.csv')
head(eco)
#Boosted GAM
#Boosted Generalized additive models
library(caret)
library(mboost)
eco = read.csv("biocap.csv")
head(eco)
#Gamboost Fitting
set.seed(123)
Train = createDataPartition(eco$BiocapacityT, p = 0.75, list = FALSE)
training = eco[Train,]
testing = eco[-Train,]
fitControl = trainControl(method = 'cv', number = 10)
Grid <- expand.grid(.mstop=seq(100,1000,100),.prune =c(5))
formula = BiocapacityT ~ Population+HDI+Grazing.Footprint+Carbon.Footprint+
Cropland+Forest.Land+Urban.Land+GDP
fit1 = train(formula, data=training, method = 'gamboost', trControl=fitControl,tuneGrid=Grid,metric='RMSE',maximize=FALSE)
print(fit1)
varImp(fit1,scale=TRUE)
v = varImp(fit1, scale = TRUE)
plot(v)
p1 = predict(fit1, newdata = testing)
p1 = predict(fit1, newdata = testing)
p = as.data.frame(p1)
test = as.data.frame(testing$BiocapacityT)
y = cbind(test,p)
colnames(y) = c("BiocapacitT","Pred")
test = as.data.frame(testing$BiocapacityT) #Actual respoinse valuse
y = cbind(test,p)
colnames(y) = c("BiocapacitT","Pred")
head(y)
cor.test(y$BiocapacitT,y$Pred)
library(Metrics)
rmse(y$BiocapacitT,y$Pred)
#Non parametric regression technique
eco = read.csv('biocap.csv')
head(eco)
set.seed(123)
Train = createDataPartition(eco$BiocapacityT, p = 0.75, list = FALSE)
testing = eco[-TTrain]
testing = eco[-Train]
install.packages('earth')
library(earth)
modfit = earth(Biocapacity~.,data = training)
modfit = earth(BiocapacityT~.,data = training)
modfit
evimp(modfit)
p = predict(modfit, newdata = testin)
p = predict(modfit, newdata = testing)
head(p)
p = as.data.frame(p)
final = cbind (tesing$Biocapacityt, p)
final = cbind (testing$Biocapacityt, p)
final = cbind (testing$T, p)
final = cbind (testing$BiocapacityT, p)
training = eco[Train,]
testing = eco[-Train]
library(earth)
modfit = earth(BiocapacityT~.,data = training)
modfit
evimp(modfit)
p = predict(modfit, newdata = testing)
head(p)
p = as.data.frame(p)
Train = createDataPartition(eco$BiocapacityT, p = 0.75, list = FALSE)
require('caret')
set.seed(123)
Train = createDataPartition(eco$BiocapacityT, p = 0.75, list = FALSE)
training = eco[Train,]
testing = eco[-Train]
library(earth)
modfit = earth(BiocapacityT~.,data = training)
modfit
evimp(modfit)
p = predict(modfit, newdata = testing)
evimp(modfit)
p = predict(modfit, newdata = testing)
modfit = earth(BiocapacityT~.,data = training)
modfit
evimp(modfit)
p = predict(modfit, newdata = testing)
#Non parametric regression technique
eco = read.csv('biocap.csv')
head(eco)
require('caret')
set.seed(123)
Train = createDataPartition(eco$BiocapacityT, p = 0.75, list = FALSE)
training = eco[Train,]
testing = eco[-Train]
library(earth)
modfit = earth(BiocapacityT~.,data = training)
modfit
evimp(modfit)
p = predict(modfit, newdata = testing)
p = predict(modfit, newdata = testing)
